{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise on Stateful Recurrent Neural Network: SOLUTION\n",
    "\n",
    "**In this exercise we want to use an improved RNN model for predicting if a ice cream store has ice on stock today. We only can use the past weather to make our predictions and hope that the ice stock today depends on the weather in the past couple of days.** \n",
    "\n",
    "**The weather is described by 3 states: 0=sunny, 1=cloudy and 2=rainy. People only buy ice when its sunny and the ice cream stand has an unknow stock of ice and reorders sometimes (unknown policy but we hope it depends on the weather).\n",
    "Unfortunately, we are quite busy with working so we can only remember the weather of the last 2 days - for that reason our lookback is only 2 days.**\n",
    "\n",
    "**To improve the simple RNN model we will use a stateful RNN model.  This means we will pass the learned hidden state into the next mini-batch connecting to the continuation of the sequence (not reset it to zero!). (For prediction with this stateful RNN we need to work on the test data with the same minibatch size as we have used for training). To work with a stateful RNN model we need to prepare our mini-batches in a special way - the first example of the fist batch has to be connected to the first example of the second batch and so on (see lecture slides).**  \n",
    "**The idea  of passing the current hidden state into the next mini-batch is, that we can learn something from the past of the sequence that is further behind than only two steps (the past is summerized in the current hidden state).**\n",
    "\n",
    "\n",
    "**a) Look at RNN model definition, the data preparation, and the model training, what is different compared to the simple RNN?**\n",
    "            \n",
    "Solution:\n",
    "\n",
    "In the model definition we need to set the argument stateful=True in the SimpleRNN layer.\n",
    "We need to prepare stateful mini-batches that connects sequence parts of the same sequence in the right order. We need to do that for the train, validation and test set seperately. During training we must not shuffle, since we must not destroy the order of the stateful batches.  After each epoch we call model.reset_states()  since we need to reset the hidden states after each epoch because there is no connection between last and first minibatch\n",
    "\n",
    "\n",
    "**b) Take the trained model and predict the first two examples of the test set. What are the probabilities for ice/no-ice for this two examples?**   \n",
    "\n",
    "Solution:\n",
    "\n",
    "probability vector for outcome ice/no-ice that we get for first example: 0.22169746 (no-ice) 0.77830255 (ice)\n",
    "\n",
    "probability vector for outcome ice/no-ice that we get for first example: 0.3353621 (no-ice)  0.66463786 (ice)\n",
    "\n",
    "**c) Complete the code to do the prediction by \"hand/numpy\" using the extracted weight matrices. (We use model.get_weights() to get the learned weights.) Which state-values do we need to give the in-coming hidden state have for example 1 and for example 2 of the test data? Do we get the same probability vectors as we got it with model.predict?**\n",
    "\n",
    "Solution:\n",
    "\n",
    "For the first example of the test data we initialize the incoming hidden state with zeros.\n",
    "For the second example of the test data, which is the continuation of the first example, we use the hidden state resulting from the first example - in this manner we do a stateful RNN prediction.\n",
    "\n",
    "We get indeed the same probability vectors like we got with model.predict showing the predict function does what we expected it to do:\n",
    "\n",
    "probability vector for outcome ice/no-ice that we get for first example: 0.22169746 (no-ice) 0.77830255 (ice)\n",
    "\n",
    "probability vector for outcome ice/no-ice that we get for first example: 0.3353621 (no-ice)  0.66463786 (ice)\n",
    "\n",
    "\n",
    "**d) Assess the performance of the stateful RNN model on the test data set. How does the achieved accuracy compare to the accuracy you have achieved with a simple RNN model?**\n",
    "\n",
    "Solution:\n",
    "\n",
    "We achieve with the stateful RNN an accuracy above the accuracy we saw with the simple RNN. With this\n",
    "we showed that in our case stateful RNN outperforms a simple RNN model.\n",
    "\n",
    "**e) Explain why the stateful RNN model does outperform the simple RNN model in our example. (Hint: remember the data generating process) How could you improve the performance of the simple RNN model? Play around with the code to check your ideas.**\n",
    "\n",
    "Solution:\n",
    "\n",
    "Since the data are simulated, we know the data generating process and we know that the ice stock depends on more than the last 2 days. Since the stateful RNN transfers the hidden state from the previous sequence part to the next minibatch we can make use of more than 2 days in the past which helped to improve the performance. To get a better performance with the simple RNN we could enlarge the used sequence. For this purpose we can e.g. use lookback=4 instead of lookback=2. We can indeed see that with a longer lookback the performance of the simple RNN improves and gets close to the corresponding stateful RNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "tf.__version__, sys.version_info\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    Xs = np.array(np.random.choice(3, size=(size,))) #Random Weather\n",
    "    Y = []\n",
    "    ice = 2 # stock of icecream at start\n",
    "    for t,x in enumerate(Xs):\n",
    "        # (t-3) >= 0 the first ice cream could be delivered on day 3\n",
    "        # Xs[t - 3] claudy three days before today => we ordered ice cream\n",
    "        # ice < 2 not full\n",
    "        if (t - 3) >= 0 and Xs[t - 3] == 1 and ice < 2: \n",
    "            ice += 1\n",
    "        if x == 0: # It is sunny we therefore sell ice, if we have\n",
    "            if ice > 0: # We have ice cream\n",
    "                ice -= 1\n",
    "        if ice > 0: #We are not out of stock\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "    return Xs, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating the data and split it to a train valid and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = gen_data(40000) \n",
    "\n",
    "lookback=2\n",
    "\n",
    "X_tr = X[0:20000]\n",
    "Y_tr = Y[0:20000]\n",
    "idx=np.arange(0, len(X_tr),lookback)\n",
    "X_train=np.zeros((len(idx),lookback))\n",
    "Y_train=np.zeros((len(idx),1))\n",
    "for i in range(0,len(idx)-1):\n",
    "    X_train[i]=X_tr[idx[i]:idx[i+1]]\n",
    "    Y_train[i]=Y_tr[idx[i]+lookback]\n",
    "\n",
    "X_va = X[20000:30000]\n",
    "Y_va = Y[20000:30000]\n",
    "idx=np.arange(0, len(X_va),lookback)\n",
    "X_valid=np.zeros((len(idx),lookback))\n",
    "Y_valid=np.zeros((len(idx),1))\n",
    "for i in range(0,len(idx)-1):\n",
    "    X_valid[i]=X_va[idx[i]:idx[i+1]]\n",
    "    Y_valid[i]=Y_va[idx[i]+lookback]\n",
    "\n",
    "X_te = X[30000:40000]\n",
    "Y_te = Y[30000:40000]\n",
    "idx=np.arange(0, len(X_te),lookback)\n",
    "X_test=np.zeros((len(idx),lookback))\n",
    "Y_test=np.zeros((len(idx),1))\n",
    "for i in range(0,len(idx)-1):\n",
    "    X_test[i]=X_te[idx[i]:idx[i+1]]\n",
    "    Y_test[i]=Y_te[idx[i]+lookback]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "(10000, 1)\n",
      "(5000, 2)\n",
      "(5000, 1)\n",
      "(5000, 2)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(Y_valid.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting to one hot encoding for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "X_train=to_categorical(X_train,3)\n",
    "Y_train=to_categorical(Y_train,2)\n",
    "\n",
    "X_valid=to_categorical(X_valid,3)\n",
    "Y_valid=to_categorical(Y_valid,2)\n",
    "\n",
    "X_test=to_categorical(X_test,3)\n",
    "Y_test=to_categorical(Y_test,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2, 3)\n",
      "(10000, 2)\n",
      "(5000, 2, 3)\n",
      "(5000, 2)\n",
      "(5000, 2, 3)\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(Y_valid.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare stateful batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0  200  400  600  800 1000 1200 1400 1600 1800 2000 2200 2400 2600\n",
      " 2800 3000 3200 3400 3600 3800 4000 4200 4400 4600 4800 5000 5200 5400\n",
      " 5600 5800 6000 6200 6400 6600 6800 7000 7200 7400 7600 7800 8000 8200\n",
      " 8400 8600 8800 9000 9200 9400 9600 9800    1  201  401  601  801 1001\n",
      " 1201 1401 1601 1801 2001 2201 2401 2601 2801 3001 3201 3401 3601 3801\n",
      " 4001 4201 4401 4601 4801 5001 5201 5401 5601 5801 6001 6201 6401 6601\n",
      " 6801 7001 7201 7401 7601 7801 8001 8201 8401 8601 8801 9001 9201 9401\n",
      " 9601 9801]\n"
     ]
    }
   ],
   "source": [
    "batch_s=50\n",
    "#first create stateful mini-batches from the training data\n",
    "batches=np.int(len(X_train)/batch_s)\n",
    "idx=np.arange(0, batches*batch_s,batches)\n",
    "for i in range(1,batches):\n",
    "    idx=np.append(idx,np.arange(0, batches*batch_s,batches)+i)\n",
    "print(idx[0:100])\n",
    "X_train_stateful=np.zeros((len(X_train),lookback,3))\n",
    "for i in range(0,len(idx)):\n",
    "    X_train_stateful[i]=X_train[idx[i]]\n",
    "Y_train_stateful=np.zeros((len(Y_train),2))\n",
    "for i in range(0,len(idx)):\n",
    "    Y_train_stateful[i]=Y_train[idx[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now create stateful mini-batches from the validation data\n",
    "batches=np.int(len(X_valid)/batch_s)\n",
    "idx=np.arange(0, batches*batch_s,batches)\n",
    "for i in range(1,batches):\n",
    "    idx=np.append(idx,np.arange(0, batches*batch_s,batches)+i)\n",
    "X_valid_stateful=np.zeros((len(X_valid),lookback,3))\n",
    "for i in range(0,len(idx)):\n",
    "    X_valid_stateful[i]=X_valid[idx[i]]\n",
    "Y_valid_stateful=np.zeros((len(Y_valid),2))\n",
    "for i in range(0,len(idx)):\n",
    "    Y_valid_stateful[i]=Y_valid[idx[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the stateful RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "name = 'RNN_stateful'\n",
    "\n",
    "model.add(SimpleRNN(4, batch_input_shape=(50,lookback, 3),stateful=True))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (50, 4)                   32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (50, 2)                   10        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (50, 2)                   0         \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "50/50 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7252042293548584, 0.5]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train_stateful[0:50],Y_train_stateful[0:50],batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3813641  0.6186359 ]\n",
      " [0.39468852 0.6053115 ]\n",
      " [0.39468852 0.6053115 ]\n",
      " [0.51871794 0.4812821 ]\n",
      " [0.51871794 0.4812821 ]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_train_stateful[0:50],batch_size=50)[0:5])\n",
    "print(Y_train_stateful[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the stateful RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7009 - acc: 0.5217 - val_loss: 0.6795 - val_acc: 0.5836\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6803 - acc: 0.5866 - val_loss: 0.6719 - val_acc: 0.5980\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6733 - acc: 0.5904 - val_loss: 0.6665 - val_acc: 0.5980\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6676 - acc: 0.5920 - val_loss: 0.6623 - val_acc: 0.6092\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6637 - acc: 0.6106 - val_loss: 0.6607 - val_acc: 0.6258\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6617 - acc: 0.6224 - val_loss: 0.6600 - val_acc: 0.6280\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6602 - acc: 0.6218 - val_loss: 0.6588 - val_acc: 0.6294\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6584 - acc: 0.6249 - val_loss: 0.6568 - val_acc: 0.6300\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6555 - acc: 0.6283 - val_loss: 0.6525 - val_acc: 0.6360\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6468 - acc: 0.6371 - val_loss: 0.6334 - val_acc: 0.6544\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6099 - acc: 0.6648 - val_loss: 0.5741 - val_acc: 0.7000\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5667 - acc: 0.7078 - val_loss: 0.5416 - val_acc: 0.7326\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5453 - acc: 0.7274 - val_loss: 0.5272 - val_acc: 0.7406\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5362 - acc: 0.7312 - val_loss: 0.5209 - val_acc: 0.7420\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5321 - acc: 0.7328 - val_loss: 0.5179 - val_acc: 0.7424\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5300 - acc: 0.7351 - val_loss: 0.5163 - val_acc: 0.7428\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5288 - acc: 0.7348 - val_loss: 0.5153 - val_acc: 0.7436\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5281 - acc: 0.7334 - val_loss: 0.5145 - val_acc: 0.7432\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5276 - acc: 0.7329 - val_loss: 0.5137 - val_acc: 0.7438\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5272 - acc: 0.7335 - val_loss: 0.5129 - val_acc: 0.7430\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5267 - acc: 0.7330 - val_loss: 0.5122 - val_acc: 0.7444\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5263 - acc: 0.7333 - val_loss: 0.5115 - val_acc: 0.7440\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5258 - acc: 0.7333 - val_loss: 0.5110 - val_acc: 0.7442\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5252 - acc: 0.7330 - val_loss: 0.5106 - val_acc: 0.7426\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5247 - acc: 0.7323 - val_loss: 0.5102 - val_acc: 0.7426\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5241 - acc: 0.7322 - val_loss: 0.5099 - val_acc: 0.7426\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5235 - acc: 0.7318 - val_loss: 0.5095 - val_acc: 0.7424\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5230 - acc: 0.7331 - val_loss: 0.5091 - val_acc: 0.7432\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5225 - acc: 0.7315 - val_loss: 0.5086 - val_acc: 0.7440\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5220 - acc: 0.7316 - val_loss: 0.5081 - val_acc: 0.7450\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    history1 = model.fit(X_train_stateful, Y_train_stateful, \n",
    "                        epochs=1, \n",
    "                        batch_size=50, \n",
    "                        verbose=2, \n",
    "                        validation_data=(X_valid_stateful,Y_valid_stateful),\n",
    "                        shuffle=False) # since stateful batches are ordered, we must not shuffle\n",
    "    model.reset_states()  \n",
    "    # we need to reset the hidden states after each epoch \n",
    "    # since there is no connection between last and first minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the training is completed we extract the learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.46829993, -0.27448082,  0.49198586,  0.18986978],\n",
       "        [-0.07143547, -0.66210234, -0.05878811, -0.4841847 ],\n",
       "        [-0.10000977, -0.64688206, -0.13557   , -0.382531  ]],\n",
       "       dtype=float32),\n",
       " array([[-0.5852696 , -0.13081849,  0.42224163,  0.7146077 ],\n",
       "        [ 1.0864695 , -0.57855856,  0.5216885 , -0.13379107],\n",
       "        [ 0.84883505,  0.6864449 , -0.44127634,  0.44790152],\n",
       "        [-0.47710732,  0.72620666,  0.7077398 ,  0.24581367]],\n",
       "       dtype=float32),\n",
       " array([ 0.17804326,  0.24491675,  0.2880884 , -0.19558866], dtype=float32),\n",
       " array([[ 0.37118736, -0.96871555],\n",
       "        [ 1.1224096 , -0.25177336],\n",
       "        [ 0.1929656 , -0.647232  ],\n",
       "        [-0.49972147, -0.40730447]], dtype=float32),\n",
       " array([-0.17113112,  0.17113112], dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1=np.row_stack(model.get_weights()[0:2])\n",
    "b1=model.get_weights()[2]\n",
    "W2=model.get_weights()[3]\n",
    "b2=model.get_weights()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46829993, -0.27448082,  0.49198586,  0.18986978],\n",
       "       [-0.07143547, -0.66210234, -0.05878811, -0.4841847 ],\n",
       "       [-0.10000977, -0.64688206, -0.13557   , -0.382531  ],\n",
       "       [-0.5852696 , -0.13081849,  0.42224163,  0.7146077 ],\n",
       "       [ 1.0864695 , -0.57855856,  0.5216885 , -0.13379107],\n",
       "       [ 0.84883505,  0.6864449 , -0.44127634,  0.44790152],\n",
       "       [-0.47710732,  0.72620666,  0.7077398 ,  0.24581367]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 # stacked matrices of hidden and input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the test data for a stateful RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the test data for a stateful RNN model\n",
    "batch_s=50\n",
    "batches=np.int(len(X_test)/batch_s)\n",
    "idx=np.arange(0, batches*batch_s,batches)\n",
    "for i in range(1,batches):\n",
    "    idx=np.append(idx,np.arange(0, batches*batch_s,batches)+i)\n",
    "\n",
    "X_test_stateful=np.zeros((len(X_test),lookback,3))\n",
    "for i in range(0,len(idx)):\n",
    "    X_test_stateful[i]=X_test[idx[i]]\n",
    "Y_test_stateful=np.zeros((len(Y_test),2))\n",
    "for i in range(0,len(idx)):\n",
    "    Y_test_stateful[i]=Y_test[idx[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the prediction on the first two examples of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "[0.22169746 0.77830255]\n",
      "[0.3353621  0.66463786]\n"
     ]
    }
   ],
   "source": [
    "# reset the hidden state to zero\n",
    "model.reset_states()\n",
    "# predict the first two mini-batches (each has size 50)\n",
    "y_pred1=model.predict(X_test_stateful[0:100],batch_size=50)\n",
    "print(y_pred1.shape) # we get for each time point the 2dim prediction\n",
    "# check the prediction of the first instance in minibatch 1 and in minibatch 2 (each mini-batch has size 50):\n",
    "# the hidden state gets passed from the first instance in minibatch 1 to first instance in minibatch 2\n",
    "# below we will do this by hand and check if we get same predictions\n",
    "print(y_pred1[0])\n",
    "print(y_pred1[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One forwardpass of a stateful RNN in numpy by \"hand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first determine the prediction of the first instance of mini-batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the ingoing hidden state for the first example of the test data:\n",
    "h0=np.array((0,0,0,0),dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1=np.tanh(np.matmul(np.concatenate((X_test_stateful[0][0],h0)),W1)+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2=np.tanh(np.matmul(np.concatenate((X_test_stateful[0][1],h1)),W1)+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22169747, 0.77830253])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z=np.matmul(h2,W2)+b2\n",
    "np.exp(Z)/np.sum(np.exp(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22169747, 0.77830253])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same again but this time with a for loop to go over the elements of a sequence\n",
    "# initialize hidden state of first(!) mini-batch with zeros\n",
    "ht_m1=np.array((0,0,0,0),dtype=\"float32\") \n",
    "\n",
    "for i in range(0,lookback):\n",
    "    ht_m1=np.tanh(np.matmul(np.concatenate((X_test_stateful[0][i],ht_m1)),W1)+b1)\n",
    "Z=np.matmul(ht_m1,W2)+b2\n",
    "np.exp(Z)/np.sum(np.exp(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now determine the prediction of the first instance of mini-batch 2 (stateful connected to first instance of mini-batch 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3353621, 0.6646379])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here to define the incoming hidden state for the second example of the test data\n",
    "# keep hidden state from last mini-batch\n",
    "ht_m2 = ht_m1\n",
    "for i in range(0,lookback):\n",
    "    ht_m2=np.tanh(np.matmul(np.concatenate((X_test_stateful[50][i],ht_m2)),W1)+b1)\n",
    "Z=np.matmul(ht_m2,W2)+b2\n",
    "np.exp(Z)/np.sum(np.exp(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we get same result as we got with predict showing that the stateful RNN is doing what we expect it to do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the Performance of the stateful RNN is better than the simple RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1314  760]\n",
      " [ 545 2381]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.739"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reset_states()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred=model.predict(X_test_stateful,batch_size=50,)\n",
    "print(confusion_matrix(np.argmax(Y_test_stateful,axis=1), np.argmax(pred,axis=1)))\n",
    "np.sum(np.argmax(pred,axis=1)==np.argmax(Y_test_stateful,axis=1))/len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we showed that stateful RNN can improve the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
